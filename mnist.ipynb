{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit ('base': conda)",
   "display_name": "Python 3.6.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "81296e002652460a41f77fd7cdff8cbeebfc284ce4c0083fdeb0e3a011fa15b5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import struct\n",
    "\n",
    "from enum import Enum\n",
    "from pathlib import Path"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "source": [
    "#### NetType为网络类型\n",
    "+ Fitting为曲线拟合\n",
    "+ BinaryClassifier为二分类\n",
    "+ MultipleClassifier为多分类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetType(Enum):\n",
    "    Fitting = 1,\n",
    "    BinaryClassifier = 2,\n",
    "    MultipleClassifier = 3"
   ]
  },
  {
   "source": [
    "#### InitialMethod为初始化方法\n",
    "+ Zero 初始化为0\n",
    "+ Normal 为正态随机分布初始化\n",
    "+ Xavier Xavier初始化方法来源于论文《[Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?source=post_page---------------------------)》，主要的目标就是使得每一层输出的方差应该尽量相等。\n",
    "+ MSRA MSRA初始化方法来源于论文《[Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852.pdf)》，是一个均值为 $0$ 方差为 $2/n$ 的高斯分布。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialMethod(Enum):\n",
    "    Zero = 0,\n",
    "    Normal = 1,\n",
    "    Xavier = 2,\n",
    "    MSRA = 3"
   ]
  },
  {
   "source": [
    "#### WeightsBias用于设置权重矩阵和偏移矩阵的值"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsBias_1_0(object):\n",
    "    def __init__(self, n_input, n_output, init_method, eta):\n",
    "        self.num_input = n_input        # 输入个数\n",
    "        self.num_output = n_output      # 输出个数\n",
    "        self.init_method = init_method  # 初始化方法\n",
    "        self.eta = eta                  # 学习率\n",
    "        self.initial_value_filename = str.format(\"w_{0}_{1}_{2}_init\", self.num_input, self.num_output, self.init_method.name)\n",
    "\n",
    "    def InitializeWeights(self, folder, create_new):\n",
    "        self.folder = folder\n",
    "        if create_new:\n",
    "            self.__CreateNew()\n",
    "        else:\n",
    "            self.__LoadExistingParameters()\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.dB = np.zeros(self.B.shape)\n",
    "\n",
    "    def __CreateNew(self):\n",
    "        self.W, self.B = WeightsBias_1_0.InitialParameters(self.num_input, self.num_output, self.init_method)\n",
    "        self.__SaveInitialValue()\n",
    "        \n",
    "    def __LoadExistingParameters(self):\n",
    "        file_name = str.format(\"{0}/{1}.npz\", self.folder, self.initial_value_filename)\n",
    "        w_file = Path(file_name)\n",
    "        if w_file.exists():\n",
    "            self.__LoadInitialValue()\n",
    "        else:\n",
    "            self.__CreateNew()\n",
    "\n",
    "    def Update(self):\n",
    "        self.W = self.W - self.eta * self.dW\n",
    "        self.B = self.B - self.eta * self.dB\n",
    "\n",
    "    def __SaveInitialValue(self):\n",
    "        file_name = str.format(\"{0}/{1}.npz\", self.folder, self.initial_value_filename)\n",
    "        np.savez(file_name, weights=self.W, bias=self.B)\n",
    "\n",
    "    def __LoadInitialValue(self):\n",
    "        file_name = str.format(\"{0}/{1}.npz\", self.folder, self.initial_value_filename)\n",
    "        data = np.load(file_name)\n",
    "        self.W = data[\"weights\"]\n",
    "        self.B = data[\"bias\"]\n",
    "\n",
    "    def SaveResultValue(self, folder, name):\n",
    "        file_name = str.format(\"{0}/{1}.npz\", folder, name)\n",
    "        np.savez(file_name, weights=self.W, bias=self.B)\n",
    "\n",
    "    def LoadResultValue(self, folder, name):\n",
    "        file_name = str.format(\"{0}/{1}.npz\", folder, name)\n",
    "        data = np.load(file_name)\n",
    "        self.W = data[\"weights\"]\n",
    "        self.B = data[\"bias\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def InitialParameters(num_input, num_output, method):\n",
    "        if method == InitialMethod.Zero:\n",
    "            W = np.zeros((num_input, num_output))\n",
    "        elif method == InitialMethod.Normal:\n",
    "            W = np.random.normal(size=(num_input, num_output))\n",
    "        elif method == InitialMethod.MSRA:\n",
    "            W = np.random.normal(0, np.sqrt(2/num_output), size=(num_input, num_output))\n",
    "        elif method == InitialMethod.Xavier:\n",
    "            W = np.random.uniform(-np.sqrt(6/(num_output+num_input)),\n",
    "                                  np.sqrt(6/(num_output+num_input)),\n",
    "                                  size=(num_input, num_output))\n",
    "        B = np.zeros((1, num_output))\n",
    "        return W, B"
   ]
  },
  {
   "source": [
    "#### 损失函数\n",
    "+ MSE 均方差损失函数(Mean Squared Error) $loss(w,b) = \\frac{1}{2}(z_i - y_i)^2$\n",
    "+ CE2 二分类交叉熵损失函数(Cross Entropy) $loss(w,b) = -[yln(a) + (1 - y)ln(1 - a)]$\n",
    "+ CE3 多分类交叉熵损失函数(Cross Entropy) $loss(w,b) = -\\sum_{i = 1}^{m}y_iln(a_i)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction_1_1(object):\n",
    "    def __init__(self, net_type):\n",
    "        self.net_type = net_type\n",
    "\n",
    "    def CheckLoss(self, A, Y):\n",
    "        m = Y.shape[0]\n",
    "        if self.net_type == NetType.Fitting:\n",
    "            loss = self.MSE(A, Y, m)\n",
    "        elif self.net_type == NetType.BinaryClassifier:\n",
    "            loss = self.CE2(A, Y, m)\n",
    "        elif self.net_type == NetType.MultipleClassifier:\n",
    "            loss = self.CE3(A, Y, m)\n",
    "        return loss\n",
    "\n",
    "    def MSE(self, A, Y, count):\n",
    "        p1 = A - Y\n",
    "        LOSS = np.multiply(p1, p1)\n",
    "        loss = LOSS.sum()/count/2\n",
    "        return loss\n",
    "\n",
    "    def CE2(self, A, Y, count):\n",
    "        p1 = 1 - Y\n",
    "        p2 = np.log(1 - A)\n",
    "        p3 = np.log(A)\n",
    "\n",
    "        p4 = np.multiply(p1 ,p2)\n",
    "        p5 = np.multiply(Y, p3)\n",
    "\n",
    "        LOSS = np.sum(-(p4 + p5))\n",
    "        loss = LOSS / count\n",
    "        return loss\n",
    "\n",
    "    def CE3(self, A, Y, count):\n",
    "        p1 = np.log(A)\n",
    "        p2 =  np.multiply(Y, p1)\n",
    "        LOSS = np.sum(-p2, keepdims=False) \n",
    "        loss = LOSS / count\n",
    "        return loss"
   ]
  },
  {
   "source": [
    "#### 超参数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters_3_0(object):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_output, \n",
    "                 eta=0.1, max_epoch=10000, batch_size=5, eps = 0.1,\n",
    "                 net_type = NetType.Fitting,\n",
    "                 init_method = InitialMethod.Xavier):\n",
    "\n",
    "        self.num_input = n_input         # 输入的个数\n",
    "        self.num_hidden1 = n_hidden1     # 第一个隐藏层神经元个数\n",
    "        self.num_hidden2 = n_hidden2     # 第一个隐藏层神经元个数\n",
    "        self.num_output = n_output       # 输出的个数\n",
    "\n",
    "        self.eta = eta                   # 学习率\n",
    "        self.max_epoch = max_epoch       # 1个epoch等于使用训练集中的全部样本训练一次\n",
    "        self.batch_size = batch_size     # 一次训练选取的样本个数\n",
    "\n",
    "        self.net_type = net_type         # 网络类型\n",
    "        self.init_method = init_method   # 初始化方法\n",
    "        self.eps = eps                   # epsilon\n",
    "\n",
    "    def toString(self):\n",
    "        title = str.format(\"bz:{0},eta:{1},ne:{2}x{3}\", self.batch_size, self.eta, self.num_hidden1, self.num_hidden2)\n",
    "        return title"
   ]
  },
  {
   "source": [
    "#### 分类函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CClassifier(object):\n",
    "    def forward(self, z):\n",
    "        pass\n",
    "\n",
    "class Logistic(CClassifier):\n",
    "    def forward(self, z):\n",
    "        a = 1.0 / (1.0 + np.exp(-z))\n",
    "        return a\n",
    "\n",
    "class Softmax(CClassifier):\n",
    "    def forward(self, z):\n",
    "        shift_z = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(shift_z)\n",
    "        a = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        return a"
   ]
  },
  {
   "source": [
    "#### 激活函数\n",
    "+ z = 本层的 wx + b 计算值矩阵\n",
    "+ a = 本层的激活函数输出值矩阵\n",
    "+ delta = 上(后)层反传回来的梯度值矩阵"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CActivator(object):\n",
    "    def forward(self, z):\n",
    "        pass\n",
    "\n",
    "    def backward(self, z, a, delta):\n",
    "        pass\n",
    "\n",
    "class Identity(CActivator):\n",
    "    def forward(self, z):\n",
    "        return z\n",
    "\n",
    "    def backward(self, z, a, delta):\n",
    "        return delta, a\n",
    "\n",
    "class Sigmoid(CActivator):\n",
    "    def forward(self, z):\n",
    "        a = 1.0 / (1.0 + np.exp(-z))\n",
    "        return a\n",
    "\n",
    "    def backward(self, z, a, delta):\n",
    "        da = np.multiply(a, 1-a)\n",
    "        dz = np.multiply(delta, da)\n",
    "        return dz, da\n",
    "\n",
    "class Tanh(CActivator):\n",
    "    def forward(self, z):\n",
    "        a = 2.0 / (1.0 + np.exp(-2*z)) - 1.0\n",
    "        return a\n",
    "\n",
    "    def backward(self, z, a, delta):\n",
    "        da = 1 - np.multiply(a, a)\n",
    "        dz = np.multiply(delta, da)\n",
    "        return dz, da\n",
    "\n",
    "class Relu(CActivator):\n",
    "    def forward(self, z):\n",
    "        a = np.maximum(z, 0)\n",
    "        return a\n",
    "\n",
    "    def backward(self, z, a, delta):\n",
    "        da = np.zeros(z.shape)\n",
    "        da[z > 0] = 1\n",
    "        dz = da * delta\n",
    "        return dz, da"
   ]
  },
  {
   "source": [
    "#### 帮助类\n",
    "+ 用于记录损失函数值极其对应的权重/迭代次数\n",
    "+ 用于图形显示损失函数值历史记录"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHistory_2_3(object):\n",
    "    def __init__(self):\n",
    "        self.loss_train = []\n",
    "        self.accuracy_train = []\n",
    "        self.iteration_seq = []\n",
    "        self.epoch_seq = []\n",
    "\n",
    "        self.loss_val = []\n",
    "        self.accuracy_val = []\n",
    "       \n",
    "    def Add(self, epoch, total_iteration, loss_train, accuracy_train, loss_vld, accuracy_vld):\n",
    "        self.iteration_seq.append(total_iteration)\n",
    "        self.epoch_seq.append(epoch)\n",
    "        self.loss_train.append(loss_train)\n",
    "        self.accuracy_train.append(accuracy_train)\n",
    "        if loss_vld is not None:\n",
    "            self.loss_val.append(loss_vld)\n",
    "        if accuracy_vld is not None:\n",
    "            self.accuracy_val.append(accuracy_vld)\n",
    "\n",
    "        return False\n",
    "\n",
    "    def ShowLossHistory(self, params, x=\"epoch\", xmin=None, xmax=None, ymin=None, ymax=None):\n",
    "        fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "        axes = plt.subplot(1,2,1)\n",
    "        if x == \"iteration\":\n",
    "            p2, = axes.plot(self.iteration_seq, self.loss_train)\n",
    "            p1, = axes.plot(self.iteration_seq, self.loss_val)\n",
    "            axes.set_xlabel(\"iteration\")\n",
    "        elif x == \"epoch\":\n",
    "            p2, = axes.plot(self.epoch_seq, self.loss_train)\n",
    "            p1, = axes.plot(self.epoch_seq, self.loss_val)\n",
    "            axes.set_xlabel(\"epoch\")\n",
    "\n",
    "        axes.legend([p1,p2], [\"validation\",\"train\"])\n",
    "        axes.set_title(\"Loss\")\n",
    "        axes.set_ylabel(\"loss\")\n",
    "        if xmin != None or xmax != None or ymin != None or ymax != None:\n",
    "            axes.axis([xmin, xmax, ymin, ymax])\n",
    "        \n",
    "        axes = plt.subplot(1,2,2)\n",
    "        if x == \"iteration\":\n",
    "            p2, = axes.plot(self.iteration_seq, self.accuracy_train)\n",
    "            p1, = axes.plot(self.iteration_seq, self.accuracy_val)\n",
    "            axes.set_xlabel(\"iteration\")\n",
    "        elif x == \"epoch\":\n",
    "            p2, = axes.plot(self.epoch_seq, self.accuracy_train)\n",
    "            p1, = axes.plot(self.epoch_seq, self.accuracy_val)\n",
    "            axes.set_xlabel(\"epoch\")\n",
    "\n",
    "        axes.legend([p1,p2], [\"validation\",\"train\"])\n",
    "        axes.set_title(\"Accuracy\")\n",
    "        axes.set_ylabel(\"accuracy\")\n",
    "        \n",
    "        title = params.toString()\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "        return title\n",
    "\n",
    "    def ShowLossHistory4(self, axes, params, xmin=None, xmax=None, ymin=None, ymax=None):\n",
    "        p2, = axes.plot(self.epoch_seq, self.loss_train)\n",
    "        p1, = axes.plot(self.epoch_seq, self.loss_val)\n",
    "        title = params.toString()\n",
    "        axes.set_title(title)\n",
    "        axes.set_xlabel(\"epoch\")\n",
    "        axes.set_ylabel(\"loss\")\n",
    "        if xmin != None and ymin != None:\n",
    "            axes.axis([xmin, xmax, ymin, ymax])\n",
    "        return title\n",
    "\n",
    "    def GetEpochNumber(self):\n",
    "        return self.epoch_seq[-1]\n",
    "\n",
    "    def GetLatestAverageLoss(self, count=10):\n",
    "        total = len(self.loss_val)\n",
    "        if count >= total:\n",
    "            count = total\n",
    "        tmp = self.loss_val[total-count:total]\n",
    "        return sum(tmp)/count\n",
    "\n",
    "    def Dump(self, file_name):\n",
    "        f = open(file_name, 'wb')\n",
    "        pickle.dump(self, f)\n",
    "\n",
    "    def Load(file_name):\n",
    "        f = open(file_name, 'rb')\n",
    "        lh = pickle.load(f)\n",
    "        return lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader_2_0(object):\n",
    "    def __init__(self, train_file, test_file):\n",
    "        self.train_file_name = train_file\n",
    "        self.test_file_name = test_file\n",
    "        self.num_train = 0        # num of training examples\n",
    "        self.num_test = 0         # num of test examples\n",
    "        self.num_validation = 0   # num of validation examples\n",
    "        self.num_feature = 0      # num of features\n",
    "        self.num_category = 0     # num of categories\n",
    "        self.XTrain = None        # training feature set\n",
    "        self.YTrain = None        # training label set\n",
    "        self.XTest = None         # test feature set\n",
    "        self.YTest = None         # test label set\n",
    "        self.XTrainRaw = None     # training feature set before normalization\n",
    "        self.YTrainRaw = None     # training label set before normalization\n",
    "        self.XTestRaw = None      # test feature set before normalization\n",
    "        self.YTestRaw = None      # test label set before normalization\n",
    "        self.XDev = None          # validation feature set\n",
    "        self.YDev = None          # validation lable set\n",
    "\n",
    "    # read data from file\n",
    "    def ReadData(self):\n",
    "        train_file = Path(self.train_file_name)\n",
    "        if train_file.exists():\n",
    "            data = np.load(self.train_file_name)\n",
    "            self.XTrainRaw = data[\"data\"]\n",
    "            self.YTrainRaw = data[\"label\"]\n",
    "            assert(self.XTrainRaw.shape[0] == self.YTrainRaw.shape[0])\n",
    "            self.num_train = self.XTrainRaw.shape[0]\n",
    "            self.num_feature = self.XTrainRaw.shape[1]\n",
    "            self.num_category = len(np.unique(self.YTrainRaw))\n",
    "            # this is for if no normalize requirment\n",
    "            self.XTrain = self.XTrainRaw\n",
    "            self.YTrain = self.YTrainRaw\n",
    "        else:\n",
    "            raise Exception(\"Cannot find train file!!!\")\n",
    "        #end if\n",
    "\n",
    "        test_file = Path(self.test_file_name)\n",
    "        if test_file.exists():\n",
    "            data = np.load(self.test_file_name)\n",
    "            self.XTestRaw = data[\"data\"]\n",
    "            self.YTestRaw = data[\"label\"]\n",
    "            assert(self.XTestRaw.shape[0] == self.YTestRaw.shape[0])\n",
    "            self.num_test = self.XTestRaw.shape[0]\n",
    "            # this is for if no normalize requirment\n",
    "            self.XTest = self.XTestRaw\n",
    "            self.YTest = self.YTestRaw\n",
    "            # in case there has no validation set created\n",
    "            self.XDev = self.XTest\n",
    "            self.YDev = self.YTest\n",
    "        else:\n",
    "            raise Exception(\"Cannot find test file!!!\")\n",
    "        #end if\n",
    "\n",
    "    # merge train/test data first, normalize, then split again\n",
    "    def NormalizeX(self):\n",
    "        x_merge = np.vstack((self.XTrainRaw, self.XTestRaw))\n",
    "        x_merge_norm = self.__NormalizeX(x_merge)\n",
    "        train_count = self.XTrainRaw.shape[0]\n",
    "        self.XTrain = x_merge_norm[0:train_count,:]\n",
    "        self.XTest = x_merge_norm[train_count:,:]\n",
    "\n",
    "    def __NormalizeX(self, raw_data):\n",
    "        temp_X = np.zeros_like(raw_data)\n",
    "        self.X_norm = np.zeros((2, self.num_feature))\n",
    "        # 按行归一化,即所有样本的同一特征值分别做归一化\n",
    "        for i in range(self.num_feature):\n",
    "            # get one feature from all examples\n",
    "            x = raw_data[:, i]\n",
    "            max_value = np.max(x)\n",
    "            min_value = np.min(x)\n",
    "            # min value\n",
    "            self.X_norm[0,i] = min_value \n",
    "            # range value\n",
    "            self.X_norm[1,i] = max_value - min_value \n",
    "            x_new = (x - self.X_norm[0,i]) / self.X_norm[1,i]\n",
    "            temp_X[:, i] = x_new\n",
    "        # end for\n",
    "        return temp_X\n",
    "\n",
    "    def NormalizeY(self, nettype, base=0):\n",
    "        if nettype == NetType.Fitting:\n",
    "            y_merge = np.vstack((self.YTrainRaw, self.YTestRaw))\n",
    "            y_merge_norm = self.__NormalizeY(y_merge)\n",
    "            train_count = self.YTrainRaw.shape[0]\n",
    "            self.YTrain = y_merge_norm[0:train_count,:]\n",
    "            self.YTest = y_merge_norm[train_count:,:]                \n",
    "        elif nettype == NetType.BinaryClassifier:\n",
    "            self.YTrain = self.__ToZeroOne(self.YTrainRaw, base)\n",
    "            self.YTest = self.__ToZeroOne(self.YTestRaw, base)\n",
    "        elif nettype == NetType.MultipleClassifier:\n",
    "            self.YTrain = self.__ToOneHot(self.YTrainRaw, base)\n",
    "            self.YTest = self.__ToOneHot(self.YTestRaw, base)\n",
    "\n",
    "    def __NormalizeY(self, raw_data):\n",
    "        assert(raw_data.shape[1] == 1)\n",
    "        self.Y_norm = np.zeros((2,1))\n",
    "        max_value = np.max(raw_data)\n",
    "        min_value = np.min(raw_data)\n",
    "        # min value\n",
    "        self.Y_norm[0, 0] = min_value \n",
    "        # range value\n",
    "        self.Y_norm[1, 0] = max_value - min_value \n",
    "        y_new = (raw_data - min_value) / self.Y_norm[1, 0]\n",
    "        return y_new\n",
    "\n",
    "    def DeNormalizeY(self, predict_data):\n",
    "        real_value = predict_data * self.Y_norm[1,0] + self.Y_norm[0,0]\n",
    "        return real_value\n",
    "\n",
    "    def __ToOneHot(self, Y, base=0):\n",
    "        count = Y.shape[0]\n",
    "        temp_Y = np.zeros((count, self.num_category))\n",
    "        for i in range(count):\n",
    "            n = (int)(Y[i,0])\n",
    "            temp_Y[i,n-base] = 1\n",
    "        return temp_Y\n",
    "\n",
    "    # for binary classifier\n",
    "    # if use tanh function, need to set negative_value = -1\n",
    "    def __ToZeroOne(Y, positive_label=1, negative_label=0, positiva_value=1, negative_value=0):\n",
    "        temp_Y = np.zeros_like(Y)\n",
    "        count = Y.shape[0]\n",
    "        for i in range(count):\n",
    "            if Y[i,0] == negative_label:     # 负类的标签设为0\n",
    "                temp_Y[i,0] = negative_value\n",
    "            elif Y[i,0] == positive_label:   # 正类的标签设为1\n",
    "                temp_Y[i,0] = positiva_value\n",
    "            # end if\n",
    "        # end for\n",
    "        return temp_Y\n",
    "\n",
    "    # normalize data by specified range and min_value\n",
    "    def NormalizePredicateData(self, X_predicate):\n",
    "        X_new = np.zeros(X_predicate.shape)\n",
    "        n_feature = X_predicate.shape[0]\n",
    "        for i in range(n_feature):\n",
    "            x = X_predicate[i,:]\n",
    "            X_new[i,:] = (x-self.X_norm[0,i])/self.X_norm[1,i]\n",
    "        return X_new\n",
    "\n",
    "    # need explicitly call this function to generate validation set\n",
    "    def GenerateValidationSet(self, k = 10):\n",
    "        self.num_validation = (int)(self.num_train / k)\n",
    "        self.num_train = self.num_train - self.num_validation\n",
    "        # validation set\n",
    "        self.XDev = self.XTrain[0:self.num_validation]\n",
    "        self.YDev = self.YTrain[0:self.num_validation]\n",
    "        # train set\n",
    "        self.XTrain = self.XTrain[self.num_validation:]\n",
    "        self.YTrain = self.YTrain[self.num_validation:]\n",
    "\n",
    "    def GetValidationSet(self):\n",
    "        return self.XDev, self.YDev\n",
    "\n",
    "    def GetTestSet(self):\n",
    "        return self.XTest, self.YTest\n",
    "\n",
    "    # 获得批样本数据\n",
    "    def GetBatchTrainSamples(self, batch_size, iteration):\n",
    "        start = iteration * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_X = self.XTrain[start:end,:]\n",
    "        batch_Y = self.YTrain[start:end,:]\n",
    "        return batch_X, batch_Y\n",
    "\n",
    "    # permutation only affect along the first axis, so we need transpose the array first\n",
    "    # see the comment of this class to understand the data format\n",
    "    def Shuffle(self):\n",
    "        seed = np.random.randint(0,100)\n",
    "        np.random.seed(seed)\n",
    "        XP = np.random.permutation(self.XTrain)\n",
    "        np.random.seed(seed)\n",
    "        YP = np.random.permutation(self.YTrain)\n",
    "        self.XTrain = XP\n",
    "        self.YTrain = YP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_file = './train-images-10'\n",
    "train_label_file = './train-labels-10'\n",
    "test_image_file = './test-images-10'\n",
    "test_label_file = './test-labels-10'\n",
    "\n",
    "class MnistImageDataReader(DataReader_2_0):\n",
    "    # mode: \"image\"=Nx1x28x28,  \"vector\"=1x784\n",
    "    def __init__(self, mode=\"image\"):\n",
    "        self.train_image_file = train_image_file\n",
    "        self.train_label_file = train_label_file\n",
    "        self.test_image_file = test_image_file\n",
    "        self.test_label_file = test_label_file\n",
    "        self.num_example = 0\n",
    "        self.num_feature = 0\n",
    "        self.num_category = 0\n",
    "        self.num_validation = 0\n",
    "        self.num_test = 0\n",
    "        self.num_train = 0\n",
    "        self.mode = mode    # image or vector\n",
    "\n",
    "    def ReadLessData(self, count):\n",
    "        self.XTrainRaw = self.__ReadImageFile(self.train_image_file)\n",
    "        self.YTrainRaw = self.__ReadLabelFile(self.train_label_file)\n",
    "        self.XTestRaw = self.__ReadImageFile(self.test_image_file)\n",
    "        self.YTestRaw = self.__ReadLabelFile(self.test_label_file)\n",
    "\n",
    "        self.XTrainRaw = self.XTrainRaw[0:count]\n",
    "        self.YTrainRaw = self.YTrainRaw[0:count]\n",
    "\n",
    "        self.num_example = self.XTrainRaw.shape[0]\n",
    "        self.num_category = (np.unique(self.YTrainRaw)).shape[0]\n",
    "        self.num_test = self.XTestRaw.shape[0]\n",
    "        self.num_train = self.num_example\n",
    "        if self.mode == \"vector\":\n",
    "            self.num_feature = 784\n",
    "        self.num_validation = 0\n",
    "\n",
    "    def ReadData(self):\n",
    "        self.XTrainRaw = self.__ReadImageFile(self.train_image_file)\n",
    "        self.YTrainRaw = self.__ReadLabelFile(self.train_label_file)\n",
    "        self.XTestRaw = self.__ReadImageFile(self.test_image_file)\n",
    "        self.YTestRaw = self.__ReadLabelFile(self.test_label_file)\n",
    "        self.num_example = self.XTrainRaw.shape[0]\n",
    "        self.num_category = (np.unique(self.YTrainRaw)).shape[0]\n",
    "        self.num_test = self.XTestRaw.shape[0]\n",
    "        self.num_train = self.num_example\n",
    "        if self.mode == \"vector\":\n",
    "            self.num_feature = 784\n",
    "        self.num_validation = 0\n",
    "\n",
    "    # output array: num_images * channel * 28 * 28\n",
    "    # due to gray image instead of color, so channel = 1\n",
    "    def __ReadImageFile(self, image_file_name):\n",
    "        # header\n",
    "        f = open(image_file_name, \"rb\")\n",
    "        a = f.read(4)\n",
    "        b = f.read(4)\n",
    "        num_images = int.from_bytes(b, byteorder='big')\n",
    "        c = f.read(4)\n",
    "        num_rows = int.from_bytes(c, byteorder='big')\n",
    "        d = f.read(4)\n",
    "        num_cols = int.from_bytes(d, byteorder='big')\n",
    "        # image data binary\n",
    "        image_size = num_rows * num_cols    # 28x28=784\n",
    "        fmt = '>' + str(image_size) + 'B'\n",
    "        image_data = np.empty((num_images,1,num_rows,num_cols)) # N x 1 x 28 x 28\n",
    "        for i in range(num_images):\n",
    "            bin_data = f.read(image_size)   # read 784 byte data for one time\n",
    "            unpacked_data = struct.unpack(fmt, bin_data)\n",
    "            array_data = np.array(unpacked_data)\n",
    "            array_data2 = array_data.reshape((1, num_rows, num_cols))\n",
    "            image_data[i] = array_data2\n",
    "        # end for\n",
    "        f.close()\n",
    "        return image_data\n",
    "\n",
    "    def __ReadLabelFile(self, lable_file_name):\n",
    "        f = open(lable_file_name, \"rb\")\n",
    "        f.read(4)\n",
    "        a = f.read(4)\n",
    "        num_labels = int.from_bytes(a, byteorder='big')\n",
    "\n",
    "        fmt = '>B'\n",
    "        label_data = np.zeros((num_labels,1))   # N x 1\n",
    "        for i in range(num_labels):\n",
    "            bin_data = f.read(1)\n",
    "            unpacked_data = struct.unpack(fmt, bin_data)[0]\n",
    "            label_data[i] = unpacked_data\n",
    "        f.close()\n",
    "        return label_data\n",
    "\n",
    "    def NormalizeX(self):\n",
    "        self.XTrain = self.__NormalizeData(self.XTrainRaw)\n",
    "        self.XTest = self.__NormalizeData(self.XTestRaw)\n",
    "\n",
    "    def __NormalizeData(self, XRawData):\n",
    "        X_NEW = np.zeros(XRawData.shape)\n",
    "        x_max = np.max(XRawData)\n",
    "        x_min = np.min(XRawData)\n",
    "        X_NEW = (XRawData - x_min)/(x_max-x_min)\n",
    "        return X_NEW\n",
    "\n",
    "    def GetBatchTrainSamples(self, batch_size, iteration):\n",
    "        start = iteration * batch_size\n",
    "        end = start + batch_size\n",
    "        if self.num_validation == 0:\n",
    "            batch_X = self.XTrain[start:end]\n",
    "            batch_Y = self.YTrain[start:end]\n",
    "        else:\n",
    "            batch_X = self.XTrain[start:end]\n",
    "            batch_Y = self.YTrain[start:end]\n",
    "        # end if\n",
    "\n",
    "        if self.mode == \"vector\":\n",
    "            return batch_X.reshape(-1, 784), batch_Y\n",
    "        elif self.mode == \"image\":\n",
    "            return batch_X, batch_Y\n",
    "\n",
    "    # recommend not use this function in DeepLearning\n",
    "    def GetValidationSet(self):\n",
    "        batch_X = self.XDev\n",
    "        batch_Y = self.YDev\n",
    "        if self.mode == \"vector\":\n",
    "            return batch_X.reshape(self.num_validation, -1), batch_Y\n",
    "        elif self.mode == \"image\":\n",
    "            return batch_X, batch_Y\n",
    "\n",
    "    def GetTestSet(self):\n",
    "        if self.mode == \"vector\":\n",
    "            return self.XTest.reshape(self.num_test,-1), self.YTest\n",
    "        elif self.mode == \"image\":\n",
    "            return self.XTest, self.YTest\n",
    "\n",
    "    def GetBatchTestSamples(self, batch_size, iteration):\n",
    "        start = iteration * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_X = self.XTest[start:end]\n",
    "        batch_Y = self.YTest[start:end]\n",
    "\n",
    "        if self.mode == \"vector\":\n",
    "            return batch_X.reshape(batch_size, -1), batch_Y\n",
    "        elif self.mode == \"image\":\n",
    "            return batch_X, batch_Y\n",
    "\n",
    "    # permutation only affect along the first axis, so we need transpose the array first\n",
    "    # see the comment of this class to understand the data format\n",
    "    # suggest to call this function for each epoch\n",
    "    def Shuffle(self):\n",
    "        seed = np.random.randint(0,100)\n",
    "        np.random.seed(seed)\n",
    "        XP = np.random.permutation(self.XTrain)\n",
    "        np.random.seed(seed)\n",
    "        YP = np.random.permutation(self.YTrain)\n",
    "        self.XTrain = XP\n",
    "        self.YTrain = YP\n",
    "        return self.XTrain, self.YTrain"
   ]
  },
  {
   "source": [
    "#### 定义神经元\n",
    "- Layers - 神经网络各层的容器，按添加顺序维护一个列表\n",
    "- Parameters - 基本参数，包括普通参数和超参\n",
    "- Loss Function - 提供计算损失函数值，存储历史记录并最后绘图的功能\n",
    "- LayerManagement() - 添加神经网络层\n",
    "- ForwardCalculation() - 调用各层的前向计算方法\n",
    "- BackPropagation() - 调用各层的反向传播方法\n",
    "- PreUpdateWeights() - 预更新各层的权重参数\n",
    "- UpdateWeights() - 更新各层的权重参数\n",
    "- Train() - 训练\n",
    "- SaveWeights() - 保存各层的权重参数\n",
    "- LoadWeights() - 加载各层的权重参数\n",
    "#### 前向传播\n",
    "$$Z1 = X \\cdot W1 + B1 \\tag{1}$$\n",
    "$$A1 = Sigmoid(Z1) \\tag{2}$$\n",
    "$$Z2 = A1 \\cdot W2 + B2 \\tag{3}$$\n",
    "$$A2 = Tanh(Z2) \\tag{4}$$\n",
    "$$Z3 = A2 \\cdot W3  + B3 \\tag{5}$$\n",
    "$$A3 = Softmax(Z3) \\tag{6}$$\n",
    "#### 反向传播\n",
    "$$dZ3 = A3-Y \\tag{7}$$\n",
    "$$dW3 = A2^T \\cdot dZ3 \\tag{8}$$\n",
    "$$dB3=dZ3 \\tag{9}$$\n",
    "$$dA2 = dZ3 \\cdot W3^T \\tag{10}$$\n",
    "$$dZ2 = dA2 \\odot (1-A2 \\odot A2) \\tag{11}$$\n",
    "$$dW2 = A1^T \\cdot dZ2 \\tag{12}$$\n",
    "$$dB2 = dZ2 \\tag{13}$$\n",
    "$$dA1 = dZ2 \\cdot W2^T \\tag{14}$$\n",
    "$$dZ1 = dA1 \\odot A1 \\odot (1-A1) \\tag{15}$$\n",
    "$$dW1 = X^T \\cdot dZ1 \\tag{16}$$\n",
    "$$dB1 = dZ1 \\tag{17}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_3_0(object):\n",
    "    def __init__(self, hp, model_name):\n",
    "        # Parameters\n",
    "        self.hp = hp\n",
    "        self.model_name = model_name\n",
    "        self.subfolder = os.getcwd() + \"/\" + self.__create_subfolder()\n",
    "        print(self.subfolder)\n",
    "        \n",
    "        # Layers\n",
    "        self.wb1 = WeightsBias_1_0(self.hp.num_input, self.hp.num_hidden1, self.hp.init_method, self.hp.eta)\n",
    "        self.wb1.InitializeWeights(self.subfolder, False)\n",
    "        self.wb2 = WeightsBias_1_0(self.hp.num_hidden1, self.hp.num_hidden2, self.hp.init_method, self.hp.eta)\n",
    "        self.wb2.InitializeWeights(self.subfolder, False)\n",
    "        self.wb3 = WeightsBias_1_0(self.hp.num_hidden2, self.hp.num_output, self.hp.init_method, self.hp.eta)\n",
    "        self.wb3.InitializeWeights(self.subfolder, False)\n",
    "\n",
    "    def __create_subfolder(self):\n",
    "        if self.model_name != None:\n",
    "            path = self.model_name.strip()\n",
    "            path = path.rstrip(\"/\")\n",
    "            isExists = os.path.exists(path)\n",
    "            if not isExists:\n",
    "                os.makedirs(path)\n",
    "            return path\n",
    "\n",
    "    # ForwardCalculation\n",
    "    def forward(self, batch_x):\n",
    "        self.Z1 = np.dot(batch_x, self.wb1.W) + self.wb1.B     #(1)\n",
    "        self.A1 = Sigmoid().forward(self.Z1)                   #(2)\n",
    "        self.Z2 = np.dot(self.A1, self.wb2.W) + self.wb2.B     #(3)\n",
    "        self.A2 = Tanh().forward(self.Z2)                      #(4)\n",
    "        self.Z3 = np.dot(self.A2, self.wb3.W) + self.wb3.B     #(5)\n",
    "        if self.hp.net_type == NetType.BinaryClassifier:       #(6)\n",
    "            self.A3 = Logistic().forward(self.Z3)              # .\n",
    "        elif self.hp.net_type == NetType.MultipleClassifier:   # .\n",
    "            self.A3 = Softmax().forward(self.Z3)               # .\n",
    "        else:                                                  # .\n",
    "            self.A3 = self.Z3                                  #(6)\n",
    "\n",
    "        self.output = self.A3\n",
    "\n",
    "    # BackPropagation\n",
    "    def backward(self, batch_x, batch_y):\n",
    "        m = batch_x.shape[0]\n",
    "        dZ3 = self.output - batch_y                            #(7)\n",
    "        self.wb3.dW = np.dot(self.A2.T, dZ3)/m                 #(8)\n",
    "        self.wb3.dB = np.sum(dZ3, axis=0, keepdims=True)/m     #(9)\n",
    "        dA2 = np.dot(dZ3, self.wb3.W.T)                        #(10)\n",
    "        dZ2,_ = Tanh().backward(None, self.A2, dA2)            #(11)\n",
    "        self.wb2.dW = np.dot(self.A1.T, dZ2)/m                 #(12)\n",
    "        self.wb2.dB = np.sum(dZ2, axis=0, keepdims=True)/m     #(13)\n",
    "        dA1 = np.dot(dZ2, self.wb2.W.T)                        #(14)\n",
    "        dZ1,_ = Sigmoid().backward(None, self.A1, dA1)         #(15)\n",
    "        self.wb1.dW = np.dot(batch_x.T, dZ1)/m                 #(16)\n",
    "        self.wb1.dB = np.sum(dZ1, axis=0, keepdims=True)/m     #(17)\n",
    "\n",
    "    # UpdateWeights\n",
    "    def update(self):\n",
    "        self.wb1.Update()\n",
    "        self.wb2.Update()\n",
    "        self.wb3.Update()\n",
    "\n",
    "    def inference(self, x):\n",
    "        self.forward(x)\n",
    "        return self.output\n",
    "\n",
    "    # Train\n",
    "    def train(self, dataReader, checkpoint, need_test):\n",
    "        t0 = time.time()\n",
    "        self.loss_trace = TrainingHistory_2_3()\n",
    "        self.loss_func = LossFunction_1_1(self.hp.net_type)\n",
    "        loss = 10\n",
    "        if self.hp.batch_size == -1:\n",
    "            self.hp.batch_size = dataReader.num_train\n",
    "        max_iteration = math.ceil(dataReader.num_train / self.hp.batch_size)\n",
    "        checkpoint_iteration = (int)(max_iteration * checkpoint)\n",
    "        need_stop = False\n",
    "        for epoch in range(self.hp.max_epoch):\n",
    "            dataReader.Shuffle()\n",
    "            for iteration in range(max_iteration):\n",
    "                # get x and y value for one sample\n",
    "                batch_x, batch_y = dataReader.GetBatchTrainSamples(self.hp.batch_size, iteration)\n",
    "                # get z from x,y\n",
    "                self.forward(batch_x)\n",
    "                # calculate gradient of w and b\n",
    "                self.backward(batch_x, batch_y)\n",
    "                # update w,b\n",
    "                self.update()\n",
    "\n",
    "                total_iteration = epoch * max_iteration + iteration\n",
    "                if (total_iteration+1) % checkpoint_iteration == 0:\n",
    "                    need_stop = self.CheckErrorAndLoss(dataReader, batch_x, batch_y, epoch, total_iteration)\n",
    "                    if need_stop:\n",
    "                        break                \n",
    "                    #end if\n",
    "                #end if\n",
    "            # end for\n",
    "            if need_stop:\n",
    "                break\n",
    "        # end for\n",
    "        self.SaveResult()\n",
    "        \n",
    "        t1 = time.time()\n",
    "        print(\"time used:\", t1 - t0)\n",
    "\n",
    "        #self.CheckErrorAndLoss(dataReader, batch_x, batch_y, epoch, total_iteration)\n",
    "        if need_test:\n",
    "            print(\"testing...\")\n",
    "            accuracy = self.Test(dataReader)\n",
    "            print(accuracy)\n",
    "        # end if\n",
    "\n",
    "    def CheckErrorAndLoss(self, dataReader, train_x, train_y, epoch, total_iteration):\n",
    "        print(\"epoch=%d, total_iteration=%d\" %(epoch, total_iteration))\n",
    "\n",
    "        # calculate train loss\n",
    "        self.forward(train_x)\n",
    "        loss_train = self.loss_func.CheckLoss(self.output, train_y)\n",
    "        accuracy_train = self.__CalAccuracy(self.output, train_y)\n",
    "        print(\"loss_train=%.6f, accuracy_train=%f\" %(loss_train, accuracy_train))\n",
    "\n",
    "        # calculate validation loss\n",
    "        vld_x, vld_y = dataReader.GetValidationSet()\n",
    "        self.forward(vld_x)\n",
    "        loss_vld = self.loss_func.CheckLoss(self.output, vld_y)\n",
    "        accuracy_vld = self.__CalAccuracy(self.output, vld_y)\n",
    "        print(\"loss_valid=%.6f, accuracy_valid=%f\" %(loss_vld, accuracy_vld))\n",
    "\n",
    "        need_stop = self.loss_trace.Add(epoch, total_iteration, loss_train, accuracy_train, loss_vld, accuracy_vld)\n",
    "        if loss_vld <= self.hp.eps:\n",
    "            need_stop = True\n",
    "        return need_stop\n",
    "\n",
    "    def Test(self, dataReader):\n",
    "        x,y = dataReader.GetTestSet()\n",
    "        self.forward(x)\n",
    "        correct = self.__CalAccuracy(self.output, y)\n",
    "        print(correct)\n",
    "\n",
    "    def __CalAccuracy(self, a, y):\n",
    "        assert(a.shape == y.shape)\n",
    "        m = a.shape[0]\n",
    "        if self.hp.net_type == NetType.Fitting:\n",
    "            var = np.var(y)\n",
    "            mse = np.sum((a-y)**2)/m\n",
    "            r2 = 1 - mse / var\n",
    "            return r2\n",
    "        elif self.hp.net_type == NetType.BinaryClassifier:\n",
    "            b = np.round(a)\n",
    "            r = (b == y)\n",
    "            correct = np.sum(r)\n",
    "            return correct/m\n",
    "        elif self.hp.net_type == NetType.MultipleClassifier:\n",
    "            ra = np.argmax(a, axis=1)\n",
    "            ry = np.argmax(y, axis=1)\n",
    "            r = (ra == ry)\n",
    "            correct = np.sum(r)\n",
    "            return correct/m\n",
    "\n",
    "    # SaveWeights\n",
    "    def SaveResult(self):\n",
    "        self.wb1.SaveResultValue(self.subfolder, \"wb1\")\n",
    "        self.wb2.SaveResultValue(self.subfolder, \"wb2\")\n",
    "        self.wb3.SaveResultValue(self.subfolder, \"wb3\")\n",
    "\n",
    "    # LoadWeights\n",
    "    def LoadResult(self):\n",
    "        self.wb1.LoadResultValue(self.subfolder, \"wb1\")\n",
    "        self.wb2.LoadResultValue(self.subfolder, \"wb2\")\n",
    "        self.wb3.LoadResultValue(self.subfolder, \"wb3\")\n",
    "\n",
    "    def ShowTrainingHistory(self, xcoord):\n",
    "        self.loss_trace.ShowLossHistory(self.hp, xcoord)\n",
    "\n",
    "    def GetTrainingTrace(self):\n",
    "        return self.loss_trace\n",
    "\n",
    "    def GetEpochNumber(self):\n",
    "        return self.loss_trace.GetEpochNumber()\n",
    "\n",
    "    def GetLatestAverageLoss(self, count=10):\n",
    "        return self.loss_trace.GetLatestAverageLoss(count)\n",
    "\n",
    "    def DumpLossHistory(self, filename):\n",
    "        return self.loss_trace.Dump(filename)"
   ]
  },
  {
   "source": [
    "#### __main__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataReader = MnistImageDataReader(mode=\"vector\")\n",
    "    dataReader.ReadData()\n",
    "    dataReader.NormalizeX()\n",
    "    dataReader.NormalizeY(NetType.MultipleClassifier, base=0)\n",
    "    dataReader.Shuffle()\n",
    "    dataReader.GenerateValidationSet(k=12)\n",
    "\n",
    "    n_input = dataReader.num_feature\n",
    "    n_hidden1 = 64\n",
    "    n_hidden2 = 16\n",
    "    n_output = dataReader.num_category\n",
    "    eta = 0.2\n",
    "    eps = 0.01\n",
    "    batch_size = 128\n",
    "    max_epoch = 20\n",
    "\n",
    "    hp = HyperParameters_3_0(\n",
    "        n_input, n_hidden1, n_hidden2, n_output, \n",
    "        eta, max_epoch, batch_size, eps, \n",
    "        NetType.MultipleClassifier, InitialMethod.Xavier)\n",
    "    net = NeuralNet_3_0(hp, \"MNIST_64_16\")\n",
    "    net.train(dataReader, 0.5, True)\n",
    "    net.ShowTrainingHistory(xcoord=\"epoch\")"
   ]
  }
 ]
}